{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec49290e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlaleal/opt/anaconda3/envs/nlp-final-proj/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6294711",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a2de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 30\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ad1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa0fa57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stances_headlines =  pd.read_csv('/Users/carlaleal/Desktop/4b/nlp-final-project/fnc-1-baseline/Transformer/train_stances.csv')\n",
    "bodies = pd.read_csv('/Users/carlaleal/Desktop/4b/nlp-final-project/fnc-1-baseline/Transformer/train_bodies.csv')\n",
    "stances_bodies = stances_headlines.merge(bodies,on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429dc8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49972"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stances_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3139de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StancesDataset(Dataset):\n",
    "    def __init__(self, headlines, bodies, stances, tokenizer, max_len):\n",
    "        self.headlines = headlines\n",
    "        self.bodies = bodies\n",
    "        self.stances = stances\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.categories = {\"unrelated\": 0, \"agree\": 1, \"discuss\": 2, \"disagree\": 3}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        headline = self.headlines[idx]\n",
    "        body = self.bodies[idx]\n",
    "        stance = self.stances[idx]\n",
    "        stance_label = self.categories[stance]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            headline,\n",
    "            body,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor([stance_label], dtype=torch.long) \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e3e83",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3eca60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanceDetectionModel(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.l1 = torch.nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _,pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                                 return_dict=False)\n",
    "        output = self.l1(pooled_output)\n",
    "        return F.softmax(output,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a86309",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea61ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size, data, model, loss_fn, optimizer,num_samples, scheduler, device='cuda:0'):\n",
    "    model = model.train()\n",
    "    model.to(device)\n",
    "    num_correct_predictions = 0\n",
    "    num_samples = len(data)\n",
    "    training_loss = []\n",
    "\n",
    "    batch_oldtime = time.time()\n",
    "    for i, input_data in enumerate(data):\n",
    "        batch_newtime = time.time()\n",
    "        batch_oldtime = batch_newtime\n",
    "        \n",
    "        input_ids = input_data['input_ids'].to(device)\n",
    "        attention_mask = input_data['attention_mask'].to(device)\n",
    "        token_type_ids = input_data['token_type_ids'].to(device)\n",
    "        labels = input_data['labels'].to(device).squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask, token_type_ids)\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        num_correct_predictions += torch.sum(preds == labels)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        training_loss.append(loss.item())\n",
    "    return num_correct_predictions.item()/num_samples, np.mean(training_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b210470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data, loss_fn,  num_samples, device='cuda:0'):\n",
    "    model = model.eval()\n",
    "    validation_losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "         for i, input_data in enumerate(data):\n",
    "            input_ids = input_data[\"input_ids\"].to(device)\n",
    "            attention_mask = input_data[\"attention_mask\"].to(device)\n",
    "            token_type_ids = input_data['token_type_ids'].to(device)\n",
    "            labels = input_data[\"labels\"].to(device).squeeze()\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "\n",
    "            loss = loss_fn(output, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            validation_losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.item() /num_samples, np.mean(validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bfd7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = StancesDataset(df['Headline'].to_numpy(),\n",
    "                        df['articleBody'].to_numpy(),\n",
    "                        df['Stance'].to_numpy(),\n",
    "                        tokenizer,\n",
    "                        max_len)\n",
    "\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size\n",
    "  )\n",
    "\n",
    "df_train, df_test = train_test_split(stances_bodies, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "train_dataloader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_dataloader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_dataloader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11984577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlaleal/opt/anaconda3/envs/nlp-final-proj/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/Users/carlaleal/opt/anaconda3/envs/nlp-final-proj/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time at start of epoch 0 is 0.000370025634765625s\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size=BATCH_SIZE\n",
    "device='cpu'\n",
    "learning_rate=0.001\n",
    "model = StanceDetectionModel(n_classes=4)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, correct_bias=False)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "oldtime = time.time()\n",
    "\n",
    "for i  in range(epochs):\n",
    "    newtime = time.time()\n",
    "    delta = newtime - oldtime\n",
    "    oldtime = newtime\n",
    "    print(f'Elapsed time at start of epoch {i} is {delta}s')\n",
    "    accuracy, mean_loss = train(batch_size,train_dataloader, model, loss_fn, optimizer, len(df_train), scheduler, device)\n",
    "    history['train_acc'].append(accuracy)\n",
    "    history['train_loss'].append(mean_loss)\n",
    "    print(f'Training accuracy at epoch {i} is {accuracy}')\n",
    "    print(f'Mean training loss at epoch {i} is {mean_loss}')\n",
    "    valAccuracy, mean_val_loss = validate(model, val_dataloader, loss_fn, len(df_val), device)\n",
    "    history['val_acc'].append(valAccuracy)\n",
    "    history['val_loss'].append(mean_val_loss)\n",
    "    print(f'Validaton accuracy at epoch {i} is {valAccuracy}')\n",
    "    print(f'Mean validation loss at epoch {i} is {mean_val_loss}')\n",
    "    \n",
    "    if valAccuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = valAccuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedaddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 100]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef2212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6dadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
