{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from utils.dataset import DataSet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "trainDataset = DataSet()\n",
    "testDataset = DataSet(\"competition_test\")\n",
    "\n",
    "FullCorpus = []\n",
    "tokenizedTrainHeadlines = []\n",
    "tokenizedTrainBodies = []\n",
    "tokenizedTestHeadlines = []\n",
    "tokenizedTestBodies = []\n",
    "\n",
    "#seen headlines and body tracker to ensure we don't have duplicates when building TF for corpus\n",
    "trainHeadlinesSeen = {}\n",
    "trainBodiesSeen = {}\n",
    "testHeadlinesSeen = {}\n",
    "testBodiesSeen = {}\n",
    "\n",
    "\n",
    "for stance in trainDataset.stances:\n",
    "    if stance['Headline'] not in trainHeadlinesSeen:\n",
    "        tokenizedHeadline = word_tokenize(stance['Headline'])\n",
    "        tokenizedTrainHeadlines.append(tokenizedHeadline)\n",
    "        trainHeadlinesSeen[stance['Headline']] = tokenizedHeadline\n",
    "    \n",
    "    if stance['Body ID'] not in trainBodiesSeen:\n",
    "        tokenizedBody = word_tokenize(trainDataset.articles[stance['Body ID']])\n",
    "        tokenizedTrainBodies.append(tokenizedBody)\n",
    "        trainBodiesSeen[stance['Body ID']] = tokenizedBody\n",
    "\n",
    "for stance in testDataset.stances:\n",
    "    if stance['Headline'] not in testHeadlinesSeen:\n",
    "        tokenizedHeadline = word_tokenize(stance['Headline'])\n",
    "        tokenizedTestHeadlines.append(tokenizedHeadline)\n",
    "        testHeadlinesSeen[stance['Headline']] = tokenizedHeadline\n",
    "    \n",
    "    if stance['Body ID'] not in testBodiesSeen:\n",
    "        tokenizedBody = word_tokenize(testDataset.articles[stance['Body ID']])\n",
    "        tokenizedTestBodies.append(tokenizedBody)\n",
    "        testBodiesSeen[stance['Body ID']] = tokenizedBody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build tokenizers and count vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([' '.join(seq) for seq in tokenizedTrainHeadlines + tokenizedTrainBodies + [\"<UNK>\"]])\n",
    "\n",
    "vocabulary=tokenizer.word_index\n",
    "vocabulary=list(vocabulary.keys())\n",
    "\n",
    "countVectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "trainCorpusBagOfWords = countVectorizer.fit_transform([' '.join(seq) for seq in tokenizedTrainHeadlines + tokenizedTrainBodies])\n",
    "\n",
    "tfVectorizer = TfidfVectorizer().fit([' '.join(seq) for seq in tokenizedTrainHeadlines + tokenizedTrainBodies + ['<UNK>']]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build feature and label vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = []\n",
    "trainLabels = []\n",
    "\n",
    "testFeatures = []\n",
    "testLabels = []\n",
    "\n",
    "# 0=unrelated 1=discuss 2=agree 3=disagree\n",
    "for stance in trainDataset.stances:\n",
    "    label = [1,0,0,0] if stance['Stance'] == 'unrelated' else [0,1,0,0] if stance['Stance'] == 'discuss' else [0,0,1,0] if stance['Stance'] == 'agree' else [0,0,0,1]  \n",
    "    trainLabels.append(label)\n",
    "    headline = [' '.join(trainHeadlinesSeen[stance['Headline']])]\n",
    "    body = [' '.join(trainBodiesSeen[stance['Body ID']])]\n",
    "    headlineTermVec = list(countVectorizer.transform(headline).toarray())[0].reshape(1, -1)\n",
    "    bodyTermVec = list(countVectorizer.transform(body).toarray())[0].reshape(1, -1)\n",
    "    \n",
    "    tfidfHeadline = tfVectorizer.transform(headline).toarray()\n",
    "    tfidfBody = tfVectorizer.transform(body).toarray()\n",
    "    tfidf_cos = cosine_similarity(tfidfHeadline, tfidfBody)[0].reshape(1, 1)\n",
    "    x = np.hstack(( tfidfHeadline, tfidfBody,tfidf_cos )).ravel()\n",
    "    trainFeatures.append(x)\n",
    "    \n",
    "for stance in testDataset.stances:\n",
    "    label = [1,0,0,0] if stance['Stance'] == 'unrelated' else [0,1,0,0] if stance['Stance'] == 'discuss' else [0,0,1,0] if stance['Stance'] == 'agree' else [0,0,0,1] \n",
    "    testLabels.append(label)\n",
    "    headline = [' '.join(testHeadlinesSeen[stance['Headline']])]\n",
    "    body = [' '.join(testBodiesSeen[stance['Body ID']])]\n",
    "    headlineTermVec = list(countVectorizer.transform(headline).toarray())[0].reshape(1, -1)\n",
    "    bodyTermVec = list(countVectorizer.transform(body).toarray())[0].reshape(1, -1)\n",
    "    \n",
    "    tfidfHeadline = tfVectorizer.transform(headline).toarray()\n",
    "    tfidfBody = tfVectorizer.transform(body).toarray()\n",
    "    tfidf_cos = cosine_similarity(tfidfHeadline, tfidfBody)[0].reshape(1, 1)\n",
    "    x = np.hstack(( tfidfHeadline, tfidfBody,tfidf_cos )).ravel()\n",
    "    testFeatures.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = np.array(trainFeatures)\n",
    "testFeatures = np.array(testFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = np.array(trainLabels)\n",
    "testLabels = np.array(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25413, 46675)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49972, 46675)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional\n",
    "\n",
    "\n",
    "#INPUT_DIM = 2*len(tokenizer.word_index) + 1\n",
    "INPUT_DIM = trainFeatures.shape[1]\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " activation_layer (Dense)    (None, 256)               11949056  \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,950,084\n",
      "Trainable params: 11,950,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelSig = Sequential()\n",
    "modelSig.add(Dense(256, activation='sigmoid', input_dim=INPUT_DIM, name=\"activation_layer\"))\n",
    "modelSig.add(Dense(4, activation='softmax', name='output_layer'))\n",
    "modelSig.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSig.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 28s 277ms/step - loss: 0.7147 - accuracy: 0.7376 - val_loss: 0.8014 - val_accuracy: 0.7220\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 18s 188ms/step - loss: 0.5709 - accuracy: 0.7819 - val_loss: 0.7471 - val_accuracy: 0.7303\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 18s 179ms/step - loss: 0.4651 - accuracy: 0.8313 - val_loss: 0.6979 - val_accuracy: 0.7586\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 18s 179ms/step - loss: 0.3804 - accuracy: 0.8690 - val_loss: 0.6489 - val_accuracy: 0.7847\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 17s 178ms/step - loss: 0.3136 - accuracy: 0.8973 - val_loss: 0.6115 - val_accuracy: 0.8112\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 19s 194ms/step - loss: 0.2616 - accuracy: 0.9169 - val_loss: 0.5777 - val_accuracy: 0.8298\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 19s 197ms/step - loss: 0.2195 - accuracy: 0.9332 - val_loss: 0.5674 - val_accuracy: 0.8314\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 20s 199ms/step - loss: 0.1868 - accuracy: 0.9460 - val_loss: 0.5440 - val_accuracy: 0.8373\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 19s 193ms/step - loss: 0.1612 - accuracy: 0.9556 - val_loss: 0.5131 - val_accuracy: 0.8466\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 18s 185ms/step - loss: 0.1403 - accuracy: 0.9632 - val_loss: 0.5043 - val_accuracy: 0.8463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b89c29d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSig.fit(trainFeatures, trainLabels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=N_EPOCHS,\n",
    "          validation_data=(testFeatures, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 4s 68ms/step - loss: 0.5043 - accuracy: 0.8463\n",
      "0.8462991118431091\n"
     ]
    }
   ],
   "source": [
    "score, acc = modelSig.evaluate(testFeatures, testLabels, batch_size=BATCH_SIZE)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " activation_layer (Dense)    (None, 256)               11949056  \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,950,084\n",
      "Trainable params: 11,950,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelRelu = Sequential()\n",
    "modelRelu.add(Dense(256, activation='relu', input_dim=INPUT_DIM, name=\"activation_layer\"))\n",
    "modelRelu.add(Dense(4, activation='softmax', name='output_layer'))\n",
    "modelRelu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRelu.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 28s 277ms/step - loss: 0.6758 - accuracy: 0.7679 - val_loss: 0.7373 - val_accuracy: 0.7636\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 18s 183ms/step - loss: 0.3261 - accuracy: 0.8935 - val_loss: 0.5967 - val_accuracy: 0.8200\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 18s 181ms/step - loss: 0.1695 - accuracy: 0.9519 - val_loss: 0.5358 - val_accuracy: 0.8328\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 18s 180ms/step - loss: 0.1034 - accuracy: 0.9737 - val_loss: 0.5038 - val_accuracy: 0.8368\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 19s 195ms/step - loss: 0.0714 - accuracy: 0.9837 - val_loss: 0.4893 - val_accuracy: 0.8439\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 20s 201ms/step - loss: 0.0535 - accuracy: 0.9875 - val_loss: 0.4963 - val_accuracy: 0.8447\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 20s 200ms/step - loss: 0.0424 - accuracy: 0.9898 - val_loss: 0.5053 - val_accuracy: 0.8448\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 19s 198ms/step - loss: 0.0349 - accuracy: 0.9918 - val_loss: 0.5166 - val_accuracy: 0.8456\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 19s 196ms/step - loss: 0.0289 - accuracy: 0.9930 - val_loss: 0.5449 - val_accuracy: 0.8417\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 19s 190ms/step - loss: 0.0249 - accuracy: 0.9942 - val_loss: 0.5448 - val_accuracy: 0.8478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b8aff310>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelRelu.fit(trainFeatures, trainLabels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=N_EPOCHS,\n",
    "          validation_data=(testFeatures, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 4s 69ms/step - loss: 0.5448 - accuracy: 0.8478\n",
      "0.8477550745010376\n"
     ]
    }
   ],
   "source": [
    "score, acc = modelRelu.evaluate(testFeatures, testLabels, batch_size=BATCH_SIZE)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " activation_layer (Dense)    (None, 256)               11949056  \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,950,084\n",
      "Trainable params: 11,950,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelTanh = Sequential()\n",
    "modelTanh.add(Dense(256, activation='tanh', input_dim=INPUT_DIM, name=\"activation_layer\"))\n",
    "modelTanh.add(Dense(4, activation='softmax', name='output_layer'))\n",
    "modelTanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTanh.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 28s 272ms/step - loss: 0.6108 - accuracy: 0.7860 - val_loss: 0.7108 - val_accuracy: 0.7683\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 18s 185ms/step - loss: 0.3077 - accuracy: 0.8955 - val_loss: 0.5923 - val_accuracy: 0.8184\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 19s 190ms/step - loss: 0.1825 - accuracy: 0.9430 - val_loss: 0.5590 - val_accuracy: 0.8184\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 18s 180ms/step - loss: 0.1233 - accuracy: 0.9643 - val_loss: 0.5466 - val_accuracy: 0.8196\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 18s 184ms/step - loss: 0.0948 - accuracy: 0.9733 - val_loss: 0.5551 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 18s 181ms/step - loss: 0.0779 - accuracy: 0.9773 - val_loss: 0.5639 - val_accuracy: 0.8172\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 18s 188ms/step - loss: 0.0683 - accuracy: 0.9795 - val_loss: 0.5581 - val_accuracy: 0.8297\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 20s 203ms/step - loss: 0.0613 - accuracy: 0.9812 - val_loss: 0.5866 - val_accuracy: 0.8189\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 21s 212ms/step - loss: 0.0572 - accuracy: 0.9816 - val_loss: 0.5927 - val_accuracy: 0.8293\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 20s 204ms/step - loss: 0.0532 - accuracy: 0.9826 - val_loss: 0.5870 - val_accuracy: 0.8360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b8c351f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTanh.fit(trainFeatures, trainLabels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=N_EPOCHS,\n",
    "          validation_data=(testFeatures, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 5s 73ms/step - loss: 0.5870 - accuracy: 0.8360\n",
      "0.8359894752502441\n"
     ]
    }
   ],
   "source": [
    "score, acc = modelTanh.evaluate(testFeatures, testLabels, batch_size=BATCH_SIZE)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the last 3 models showed Relu as the top performer it will be used in the final model, after experimenting with dropout rates and normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRelu2 = Sequential()\n",
    "modelRelu2.add(Dropout(rate=0.1, name='dropout_1'))\n",
    "modelRelu2.add(BatchNormalization(name='bn'))\n",
    "modelRelu2.add(Dense(256, activation='relu', input_dim=INPUT_DIM, name=\"activation_layer\"))\n",
    "modelRelu2.add(Dense(4, activation='softmax', name='output_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRelu2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 55s 544ms/step - loss: 0.3740 - accuracy: 0.8700 - val_loss: 0.8803 - val_accuracy: 0.7235\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 43s 434ms/step - loss: 0.0814 - accuracy: 0.9735 - val_loss: 0.9780 - val_accuracy: 0.7232\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 42s 426ms/step - loss: 0.0411 - accuracy: 0.9864 - val_loss: 0.8768 - val_accuracy: 0.8002\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 42s 427ms/step - loss: 0.0255 - accuracy: 0.9916 - val_loss: 0.9336 - val_accuracy: 0.6194\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 45s 461ms/step - loss: 0.0198 - accuracy: 0.9940 - val_loss: 0.7520 - val_accuracy: 0.7847\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 44s 451ms/step - loss: 0.0161 - accuracy: 0.9950 - val_loss: 0.7227 - val_accuracy: 0.7592\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 46s 475ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.7313 - val_accuracy: 0.7374\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 45s 461ms/step - loss: 0.0128 - accuracy: 0.9964 - val_loss: 0.8027 - val_accuracy: 0.6929\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 44s 454ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.7195 - val_accuracy: 0.7612\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 44s 450ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 0.7540 - val_accuracy: 0.7352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b9957d60>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelRelu2.fit(trainFeatures, trainLabels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=N_EPOCHS,\n",
    "          validation_data=(testFeatures, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 6s 99ms/step - loss: 0.7540 - accuracy: 0.7352\n",
      "0.7352142333984375\n"
     ]
    }
   ],
   "source": [
    "score, acc = modelRelu2.evaluate(testFeatures, testLabels, batch_size=BATCH_SIZE)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
