{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from utils.dataset import DataSet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    }
   ],
   "source": [
    "trainDataset = DataSet()\n",
    "testDataset = DataSet(\"competition_test\")\n",
    "\n",
    "FullCorpus = []\n",
    "tokenizedTrainHeadlines = []\n",
    "tokenizedTrainBodies = []\n",
    "tokenizedTestHeadlines = []\n",
    "tokenizedTestBodies = []\n",
    "\n",
    "#seen headlines and body tracker to ensure we don't have duplicates when building TF for corpus\n",
    "trainHeadlinesSeen = {}\n",
    "trainBodiesSeen = {}\n",
    "testHeadlinesSeen = {}\n",
    "testBodiesSeen = {}\n",
    "\n",
    "\n",
    "for stance in trainDataset.stances:\n",
    "    if stance['Headline'] not in trainHeadlinesSeen:\n",
    "        tokenizedHeadline = word_tokenize(stance['Headline'])\n",
    "        tokenizedTrainHeadlines.append(tokenizedHeadline)\n",
    "        trainHeadlinesSeen[stance['Headline']] = tokenizedHeadline\n",
    "    \n",
    "    if stance['Body ID'] not in trainBodiesSeen:\n",
    "        tokenizedBody = word_tokenize(trainDataset.articles[stance['Body ID']])\n",
    "        tokenizedTrainBodies.append(tokenizedBody)\n",
    "        trainBodiesSeen[stance['Body ID']] = tokenizedBody\n",
    "\n",
    "for stance in testDataset.stances:\n",
    "    if stance['Headline'] not in testHeadlinesSeen:\n",
    "        tokenizedHeadline = word_tokenize(stance['Headline'])\n",
    "        tokenizedTestHeadlines.append(tokenizedHeadline)\n",
    "        testHeadlinesSeen[stance['Headline']] = tokenizedHeadline\n",
    "    \n",
    "    if stance['Body ID'] not in testBodiesSeen:\n",
    "        tokenizedBody = word_tokenize(testDataset.articles[stance['Body ID']])\n",
    "        tokenizedTestBodies.append(tokenizedBody)\n",
    "        testBodiesSeen[stance['Body ID']] = tokenizedBody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build tokenizers and count vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([' '.join(seq) for seq in tokenizedTrainHeadlines + tokenizedTrainBodies + [\"<UNK>\"]])\n",
    "\n",
    "vocabulary=tokenizer.word_index\n",
    "vocabulary=list(vocabulary.keys())\n",
    "\n",
    "countVectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "trainCorpusBagOfWords = countVectorizer.fit_transform([' '.join(seq) for seq in tokenizedTrainHeadlines + tokenizedTrainBodies])\n",
    "\n",
    "tfVectorizer = TfidfVectorizer().fit([' '.join(seq) for seq in tokenizedTrainHeadlines + tokenizedTrainBodies + ['<UNK>']]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build feature and label vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = []\n",
    "trainLabels = []\n",
    "\n",
    "testFeatures = []\n",
    "testLabels = []\n",
    "\n",
    "# 0=unrelated 1=discuss 2=agree 3=disagree\n",
    "for stance in trainDataset.stances:\n",
    "    label = [1,0,0,0] if stance['Stance'] == 'unrelated' else [0,1,0,0] if stance['Stance'] == 'discuss' else [0,0,1,0] if stance['Stance'] == 'agree' else [0,0,0,1]  \n",
    "    trainLabels.append(label)\n",
    "    headline = [' '.join(trainHeadlinesSeen[stance['Headline']])]\n",
    "    body = [' '.join(trainBodiesSeen[stance['Body ID']])]\n",
    "    headlineTermVec = list(countVectorizer.transform(headline).toarray())[0].reshape(1, -1)\n",
    "    bodyTermVec = list(countVectorizer.transform(body).toarray())[0].reshape(1, -1)\n",
    "    \n",
    "    tfidfHeadline = tfVectorizer.transform(headline).toarray()\n",
    "    tfidfBody = tfVectorizer.transform(body).toarray()\n",
    "    tfidf_cos = cosine_similarity(tfidfHeadline, tfidfBody)[0].reshape(1, 1)\n",
    "    x = np.hstack(( tfidfHeadline, tfidfBody,tfidf_cos )).ravel()\n",
    "    trainFeatures.append(x)\n",
    "    \n",
    "for stance in testDataset.stances:\n",
    "    label = [1,0,0,0] if stance['Stance'] == 'unrelated' else [0,1,0,0] if stance['Stance'] == 'discuss' else [0,0,1,0] if stance['Stance'] == 'agree' else [0,0,0,1] \n",
    "    testLabels.append(label)\n",
    "    headline = [' '.join(testHeadlinesSeen[stance['Headline']])]\n",
    "    body = [' '.join(testBodiesSeen[stance['Body ID']])]\n",
    "    headlineTermVec = list(countVectorizer.transform(headline).toarray())[0].reshape(1, -1)\n",
    "    bodyTermVec = list(countVectorizer.transform(body).toarray())[0].reshape(1, -1)\n",
    "    \n",
    "    tfidfHeadline = tfVectorizer.transform(headline).toarray()\n",
    "    tfidfBody = tfVectorizer.transform(body).toarray()\n",
    "    tfidf_cos = cosine_similarity(tfidfHeadline, tfidfBody)[0].reshape(1, 1)\n",
    "    x = np.hstack(( tfidfHeadline, tfidfBody,tfidf_cos )).ravel()\n",
    "    testFeatures.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = np.array(trainFeatures)\n",
    "testFeatures = np.array(testFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = np.array(trainLabels)\n",
    "testLabels = np.array(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25413, 46675)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49972, 46675)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional\n",
    "\n",
    "\n",
    "#INPUT_DIM = 2*len(tokenizer.word_index) + 1\n",
    "INPUT_DIM = trainFeatures.shape[1]\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " activation_layer (Dense)    (None, 256)               11949056  \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,950,084\n",
      "Trainable params: 11,950,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelSig = Sequential()\n",
    "modelSig.add(Dense(256, activation='sigmoid', input_dim=INPUT_DIM, name=\"activation_layer\"))\n",
    "modelSig.add(Dense(4, activation='softmax', name='output_layer'))\n",
    "modelSig.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSig.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 28s 277ms/step - loss: 0.7147 - accuracy: 0.7376 - val_loss: 0.8014 - val_accuracy: 0.7220\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 18s 188ms/step - loss: 0.5709 - accuracy: 0.7819 - val_loss: 0.7471 - val_accuracy: 0.7303\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 18s 179ms/step - loss: 0.4651 - accuracy: 0.8313 - val_loss: 0.6979 - val_accuracy: 0.7586\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 18s 179ms/step - loss: 0.3804 - accuracy: 0.8690 - val_loss: 0.6489 - val_accuracy: 0.7847\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 17s 178ms/step - loss: 0.3136 - accuracy: 0.8973 - val_loss: 0.6115 - val_accuracy: 0.8112\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 19s 194ms/step - loss: 0.2616 - accuracy: 0.9169 - val_loss: 0.5777 - val_accuracy: 0.8298\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 19s 197ms/step - loss: 0.2195 - accuracy: 0.9332 - val_loss: 0.5674 - val_accuracy: 0.8314\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 20s 199ms/step - loss: 0.1868 - accuracy: 0.9460 - val_loss: 0.5440 - val_accuracy: 0.8373\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 19s 193ms/step - loss: 0.1612 - accuracy: 0.9556 - val_loss: 0.5131 - val_accuracy: 0.8466\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 18s 185ms/step - loss: 0.1403 - accuracy: 0.9632 - val_loss: 0.5043 - val_accuracy: 0.8463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b89c29d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSig.fit(trainFeatures, trainLabels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=N_EPOCHS,\n",
    "          validation_data=(testFeatures, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 4s 68ms/step - loss: 0.5043 - accuracy: 0.8463\n",
      "0.8462991118431091\n"
     ]
    }
   ],
   "source": [
    "score, acc = modelSig.evaluate(testFeatures, testLabels, batch_size=BATCH_SIZE)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " activation_layer (Dense)    (None, 256)               11949056  \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,950,084\n",
      "Trainable params: 11,950,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:19:37.309706: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "modelRelu = Sequential()\n",
    "modelRelu.add(Dense(256, activation='relu', input_dim=INPUT_DIM, name=\"activation_layer\"))\n",
    "modelRelu.add(Dense(4, activation='softmax', name='output_layer'))\n",
    "modelRelu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRelu.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 34s 335ms/step - loss: 0.6617 - accuracy: 0.7730 - val_loss: 0.7163 - val_accuracy: 0.7663\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 23s 229ms/step - loss: 0.3094 - accuracy: 0.8987 - val_loss: 0.6026 - val_accuracy: 0.8216\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 21s 209ms/step - loss: 0.1592 - accuracy: 0.9549 - val_loss: 0.5427 - val_accuracy: 0.8285\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 23s 234ms/step - loss: 0.0971 - accuracy: 0.9753 - val_loss: 0.4959 - val_accuracy: 0.8397\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 23s 237ms/step - loss: 0.0670 - accuracy: 0.9842 - val_loss: 0.4856 - val_accuracy: 0.8454\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 23s 234ms/step - loss: 0.0504 - accuracy: 0.9885 - val_loss: 0.4795 - val_accuracy: 0.8479\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 23s 234ms/step - loss: 0.0403 - accuracy: 0.9904 - val_loss: 0.5022 - val_accuracy: 0.8447\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 24s 241ms/step - loss: 0.0333 - accuracy: 0.9917 - val_loss: 0.5031 - val_accuracy: 0.8488\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 28s 282ms/step - loss: 0.0277 - accuracy: 0.9931 - val_loss: 0.5217 - val_accuracy: 0.8497\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 25s 254ms/step - loss: 0.0233 - accuracy: 0.9945 - val_loss: 0.5345 - val_accuracy: 0.8486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ae9abdc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelRelu.fit(trainFeatures, trainLabels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=N_EPOCHS,\n",
    "          validation_data=(testFeatures, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 5s 75ms/step - loss: 0.5345 - accuracy: 0.8486\n",
      "0.8485814332962036\n"
     ]
    }
   ],
   "source": [
    "score, acc = modelRelu.evaluate(testFeatures, testLabels, batch_size=BATCH_SIZE)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " activation_layer (Dense)    (None, 256)               11949056  \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,950,084\n",
      "Trainable params: 11,950,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelTanh = Sequential()\n",
    "modelTanh.add(Dense(256, activation='tanh', input_dim=INPUT_DIM, name=\"activation_layer\"))\n",
    "modelTanh.add(Dense(4, activation='softmax', name='output_layer'))\n",
    "modelTanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTanh.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 28s 272ms/step - loss: 0.6108 - accuracy: 0.7860 - val_loss: 0.7108 - val_accuracy: 0.7683\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 18s 185ms/step - loss: 0.3077 - accuracy: 0.8955 - val_loss: 0.5923 - val_accuracy: 0.8184\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 19s 190ms/step - loss: 0.1825 - accuracy: 0.9430 - val_loss: 0.5590 - val_accuracy: 0.8184\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 18s 180ms/step - loss: 0.1233 - accuracy: 0.9643 - val_loss: 0.5466 - val_accuracy: 0.8196\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 18s 184ms/step - loss: 0.0948 - accuracy: 0.9733 - val_loss: 0.5551 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 18s 181ms/step - loss: 0.0779 - accuracy: 0.9773 - val_loss: 0.5639 - val_accuracy: 0.8172\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 18s 188ms/step - loss: 0.0683 - accuracy: 0.9795 - val_loss: 0.5581 - val_accuracy: 0.8297\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 20s 203ms/step - loss: 0.0613 - accuracy: 0.9812 - val_loss: 0.5866 - val_accuracy: 0.8189\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 21s 212ms/step - loss: 0.0572 - accuracy: 0.9816 - val_loss: 0.5927 - val_accuracy: 0.8293\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 20s 204ms/step - loss: 0.0532 - accuracy: 0.9826 - val_loss: 0.5870 - val_accuracy: 0.8360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b8c351f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTanh.fit(trainFeatures, trainLabels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=N_EPOCHS,\n",
    "          validation_data=(testFeatures, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 5s 73ms/step - loss: 0.5870 - accuracy: 0.8360\n",
      "0.8359894752502441\n"
     ]
    }
   ],
   "source": [
    "score, acc = modelTanh.evaluate(testFeatures, testLabels, batch_size=BATCH_SIZE)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the last 3 models showed Relu as the top performer it will be used in the final model, after experimenting with dropout rates and normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRelu2 = Sequential()\n",
    "modelRelu2.add(Dropout(rate=0.1, name='dropout_1'))\n",
    "modelRelu2.add(BatchNormalization(name='bn'))\n",
    "modelRelu2.add(Dense(256, activation='relu', input_dim=INPUT_DIM, name=\"activation_layer\"))\n",
    "modelRelu2.add(Dense(4, activation='softmax', name='output_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRelu2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 55s 544ms/step - loss: 0.3740 - accuracy: 0.8700 - val_loss: 0.8803 - val_accuracy: 0.7235\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 43s 434ms/step - loss: 0.0814 - accuracy: 0.9735 - val_loss: 0.9780 - val_accuracy: 0.7232\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 42s 426ms/step - loss: 0.0411 - accuracy: 0.9864 - val_loss: 0.8768 - val_accuracy: 0.8002\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 42s 427ms/step - loss: 0.0255 - accuracy: 0.9916 - val_loss: 0.9336 - val_accuracy: 0.6194\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 45s 461ms/step - loss: 0.0198 - accuracy: 0.9940 - val_loss: 0.7520 - val_accuracy: 0.7847\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 44s 451ms/step - loss: 0.0161 - accuracy: 0.9950 - val_loss: 0.7227 - val_accuracy: 0.7592\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 46s 475ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.7313 - val_accuracy: 0.7374\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 45s 461ms/step - loss: 0.0128 - accuracy: 0.9964 - val_loss: 0.8027 - val_accuracy: 0.6929\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 44s 454ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.7195 - val_accuracy: 0.7612\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 44s 450ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 0.7540 - val_accuracy: 0.7352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b9957d60>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelRelu2.fit(trainFeatures, trainLabels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=N_EPOCHS,\n",
    "          validation_data=(testFeatures, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 6s 99ms/step - loss: 0.7540 - accuracy: 0.7352\n",
      "0.7352142333984375\n"
     ]
    }
   ],
   "source": [
    "score, acc = modelRelu2.evaluate(testFeatures, testLabels, batch_size=BATCH_SIZE)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Results csv for Relu model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"FinalRelu.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(testFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\"Headline\": [], \"Body ID\": [], \"Stance\": []}\n",
    "\n",
    "# 0=unrelated 1=discuss 2=agree 3=disagree\n",
    "for i, stance in enumerate(testDataset.stances):\n",
    "    result[\"Headline\"].append(stance['Headline'])\n",
    "    result[\"Body ID\"].append(stance['Body ID'])\n",
    "    prediction = predicted[i]\n",
    "    predictionIndex = np.where(prediction == np.amax(prediction))[0][0]\n",
    "    stance = 'unrelated' if predictionIndex == 0 else 'discuss' if predictionIndex == 1 else 'agree' if predictionIndex == 2 else 'disagree'\n",
    "    result[\"Stance\"].append(stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame.from_dict(result)\n",
    "    \n",
    "result.to_csv('answer.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
